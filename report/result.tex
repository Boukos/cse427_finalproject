%Write result here
\section{Results}
We evaluated the performance of our algorithm and the impact of
parameters on a subset of Netflix Prize dataset by the prediction
accuracy and the number of unpredicted ratings in the test set. We
report two accuracy measurements: the root mean swuare error(RMSE) and
the mean absolute error(MAE).

One important parameter in our algorithm is $K$, the number of similar
movies to consider in calculating ratings' predictions. We have tried
different settings of $K$ to investigate how it can affect the
performance of our algorithm.

%Table \ref{tab:errorsK} summarizes the prediction results for different Ks ranging from to.

%\begin{table}[!ht]
% \centering \begin{tabular}{|c|c|c|c|} \hline K & MAE & RMSE &
% Percentage of Unpredicted Ratings \\ \hline 20 & 0.982 & 1.322 &
% 30.36\%\\
%\hline
%50 & 0.851 & 1.136 &  12.11\%\\
%\hline
%100 & 0.798 & 1.045 & 5.04 \%
%\hline
%200 & 0.782 & 1.004 & 1.85 \% 
%\hline
%Adaptive &  \\ 
%   \hline \end{tabular} \caption{Collaborative Filtering
%(Avg. Ratings) Performance using different $K$
%values} \label{tab:errorsK}
%\end{table}
Figure ~\ref{fig:rmse} and Figure \ref{fig:mae} shows the RMSE and MAE
for our collaborative filtering algorithm using different $K$. The
error measures, RMSE and MAE, are computed for all entries in the test
set. For the entries that we could not predict (due to the small $K$),
we use their average ratings in the training set as our
prediction. The average rating per movie is calculated by a
pre-processing Map Reduce job.

The results suggest that increasing $K$ below a threshold helps
improving the overall accuracy. However, increasing the $K$ above that
threshold will descrese the accuracy. Figure~\ref{fig:rmsepred} shows
the RMSE for predicted entries only with varying $K$. This suggests
that the improvement of accuracy when we increase $K$ mainly comes the
fact that we have more entries predicted. However, further increasing
$K$ over a threshold provides only insignificant gain in ratio of
predicted entries, but will introduce large variance to the model,
which damages the overall accuracy. We have found that the ``sweet
point'' of $K$ is around 200.

We also compared different methods for prediction in Figure
~\ref{fig:rmse} and Figure ~\ref{fig:mae}, including using average
rating of the top $K$ movies and using weighted (by similarity)
average rating of the top $K$ movies. We also tried rounding the
predicted ratings to the nearest integers for both average rating and
weighted average rating. Additionally, we compare our results to two
baseline methods: the global average rating of all movies, and the
average rating of each individual movie.

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/rmse.png}
  \caption{RMSE for our collaborative filtering approach using
    different $K$ values}
  \label{fig:rmse}
\end{figure}

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/MAE.png}
  \caption{MAE for our collaborative filtering approach using
    different $K$ values}
  \label{fig:mae}
\end{figure}

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/rmsep.png}
  \caption{RMSE using weighted average prediction for predicted entries
    only with different $K$ values}
  \label{fig:rmsepred}
\end{figure}

The results suggest that using weighted average rating is better than
using unweighted average rating by only a small margin. This is due to
the fact that the variance of cosine similarities between the top $K$
movies being rated by the same user is not large. The result also
shows that rounding the predicted ratings will damage the RMSE
measurement, since RMSE is sensitive to noises and will amplify small
errors. Therefore, we only report the unrounded results below.

%% Regarding, rounding
%% the predictions, it can be noticed that rounding reduces MAE while it
%% largely increases RMSE over the movie mean baseline as shown in Figure
%% \ref{fig:rmse} since RMSE is more sensitive than MAE to outliers. If
%% rounding goes on the wrong direction, RMSE would increase. Therefore,
%% we have used unrounded version of weighted average prediction for
%% further experiments.

Figure ~\ref{fig:perc} shows the percentage of unpredicted ratings as
we varies $K$. It shows that increasing $K$ will reduce the percentage
of unpredicted ratings, since considering more similar neighbours
(movies) would increase the probability of having some of these movies
rated by the tesing user. However, this benefit will be insignificant
when $K$ is already large. For example, the percentage of unpredicted
ratings only drops from $0.84\%$ to $0.824\%$ as we increase $K$ from
$500$ to $1000$. As discussed above, this benefit will be overwhelmed
by the large variance introduced by large $K$.

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/perc.png}
  \caption{Percentage of unpredicted ratings using different $K$
    values}
  \label{fig:perc}
\end{figure}

%%Limiting m
To balance the ratio of predicted entries and the accuracy of
predicted ratings, we developed an adaptive prediction scheme. We use
two parameters to control the whole process: for a specific user $i$,
and movie $j$, we first find $K=500$ candidate movies using the above
algorithm. In the next step, instead of using all 500 similar movies,
we only use top $M$ movies that are rated by the user $i$. In the case
that the user $i$ rated less than $M$ movies among all $K=500$
candidates we use all candidates movies that are rated by user $i$.

%% However, our scheme aims at not sacrificing the accuracy of the
%% already predicted entries, the ones having many overlapping movies
%% that we can predict their ratings with using smaller $K$. Our scheme
%% is to use $K=500$ as it is the best point in terms of both accuracy
%% and percentage of missing ratings $0.84\%$, see Figure ~\ref{fig:rmse}
%% and Figure ~\ref{fig:mae}. However, we limit the number of neighbours
%% (similar movies) actually used in rating prediction to the top $M$ out
%% of these $K$ movies, where $M$ is much less than $K$.

Figure~\ref{fig:rmsem} and~\ref{fig:maem} show RMSE and MAE with
$K=500$ and varying $M$ from 5 to 100. With $M=20$ we are able to have
comparable accuracy as with $K=200$, while the comparable amount of
predicted entries as with $K=500$, See Table~\ref{tab:finres}.

%% It can be observed that having too small $M<20$
%% does not yield the best performance, we think the reason for that can
%% be that some of considered $K$ movies may have only one common user
%% rating them, having 1 cosine similarity, however, such movies with low
%% number of overlapped ratings may not be enough to have accurate
%% predictions. On the other hand increasing $M$ too much would increase
%% the error, this is reasonable and this is the motivation for using
%% $M$. Accordingly, we pick $M=20$, for our final predictions, which was
%% the best point for both Figure ~\ref{fig:rmsem} and ~\ref{fig:maem}.

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/rmsem.png}
  \caption{RMSE with varying $M$ using weighted average prediction and $K=500$.}
  \label{fig:rmsem}
\end{figure}

\begin{figure}[!ht]
  \centering \includegraphics[width=0.8\textwidth]{images/MAEm.png}
  \caption{MAE with varying $M$ using weighted average prediction and $K=500$.}
  \label{fig:maem}
\end{figure}

%%FInal Results Table
Table \ref{tab:finres} summarizes our final results for our developed
collaborative filtering approach using $K=500$ and $M=20$. Results are
presented in terms of prediction accuracy using RMSE and MAE, and for
the percentage of unpredicted ratings. The table also shows the
comparison against two baselines methods: the global mean rating and
movie mean rating. Our collaborative filtering approach have better
accuracy than the two baselines with low percentage of unpredicted
entries.

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline & RMSE & MAE & Percentage of unpredicted ratings \\ \hline
    Collaborative Filtering & \textbf{0.98099} & \textbf{0.76806} &
    0.84\%\\ \hline Global Mean Baseline & 1.08502 & 0.91094 & -
    \\ \hline Movie Mean Baseline & 1.0067 & 0.8065 & - \\ \hline
  \end{tabular}
  \caption{Result of our collaborative filtering and the comparison
    against two baseline methods.}
  \label{tab:finres}
\end{table}


\textbf{Test on my own preference:} I selected 5 movies from
the training set and rated them as shown in Table
\ref{tab:preference}. To predict ratings for movies I
haven't rated, I put my ratings in the training set and
built a testing set containings all movies I haven't rated
associated with my own userID. Then I ran the same prediction script on the new
training set and testing set, and ranked the prediction
results in decreasing order of the predicted ratings. Movies
with highest predicted ratings include \textit{Teenage
  Catgirls in Heat}, \textit{The Gauntlet} and
\textit{Nevada Smith}. I think the prediction matches my
preference well as both \textit{The Gauntlet} and
\textit{Nevada Smith} are of same type with
\textit{Mississippi Burning} which I rated high.

\begin{table}[!ht]
  \centering
  \begin{tabular}{c c}
    Movie title & My rating \\
    \hline
    \textit{Catherine the Great} & 5.0 \\
    \textit{Me \& Isaac Newton} & 4.0 \\
    \textit{Mississippi Burning} & 5.0 \\
    \textit{Uptown Girls} & 4.0 \\
    \textit{The American President} & 5.0\\
  \end{tabular}
  \caption{My ratings for five movies in the training set.}
  \label{tab:preference}
\end{table}
